# -*- coding: utf-8 -*-
"""SentimenAnalys.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LA3NYjUPLhk8zPzmFyddDQGyRGdhNL3e

Model sentimen analysis
"""

# !pip install textblob

# !pip install tensorflow

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
import nltk
import re
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB

df = pd.read_csv('/content/emotion_sentimen_dataset.csv')
df.head()

df.isnull().sum()

modus = df['Emotion'].mode()[0]
df['Emotion'] = df['Emotion'].fillna(modus)
df.isna().sum()

df.info()

df['Emotion'].value_counts()

def getSubjektivity(text):
  objek = TextBlob(text)
  level = objek.sentiment.polarity

  if level > 0:
    result = "Positive"
  elif level < 0:
    result = "Negatif"
  else:
    result = "Neutral"
  return result

getSubjektivity("i'm feel nothing")

df['Sentimen'] = df['text'].apply(getSubjektivity)
df.head(10)

# cek karakter atau puntuation text
df['text']

"""Aman dari punctuations ataupun tanda baca yang berlebih


"""

df.duplicated()

## cleaning text
import string

def clean_text(text):
    # Mengubah teks menjadi huruf kecil
    text = text.lower()
    # Menghapus URL
    text = re.sub(r"http\S+|www\S+|https\S+", '', text, flags=re.MULTILINE)
    # Menghapus angka
    text = re.sub(r'\d+', '', text)
    # Menghapus tanda baca
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Menghapus spasi tambahan
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['text'] = df['text'].apply(clean_text)

# membuat fitur dan label
X_feature = df['text']
y_label = df['Sentimen']

# Tokenisasi teks
# Dikurangi, awalnya 10.000
tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
tokenizer.fit_on_texts(X_feature)

# Mengubah teks menjadi urutan angka
X_seq = tokenizer.texts_to_sequences(X_feature)

# Padding untuk membuat panjang teks seragam
max_length = 100
X_padded = pad_sequences(X_seq, maxlen=max_length, padding='post', truncating='post')

# Pastikan label dikonversi ke one-hot encoding
y_onehot = pd.get_dummies(y_label).values

x_train,x_test,y_train,y_test = train_test_split(X_padded, y_onehot, test_size=0.3,random_state=42)

# Model dengan LSTM
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32),
    tf.keras.layers.Embedding(input_dim=10000, output_dim=64, input_length=max_length),
    tf.keras.layers.Bidirectional(LSTM(64, return_sequences=True)),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Bidirectional(LSTM(32)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(3, activation='softmax')
])
# Kompilasi model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

class StopAtAccuracy(tf.keras.callbacks.Callback):
    def __init__(self, target_accuracy=0.95):
        super(StopAtAccuracy, self).__init__()
        self.target_accuracy = target_accuracy

    def on_epoch_end(self, epoch, logs=None):
        acc = logs.get("accuracy")

        if acc >= self.target_accuracy:
            print(f"\nEpoch {epoch+1}: Target accuracy reached. Stopping training.")
            self.model.stop_training = True

early_stopping = StopAtAccuracy(target_accuracy=0.95)

# Melatih model
history = model.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=10,
    callbacks=[early_stopping]
)

# Evaluasi model
loss, accuracy = model.evaluate(x_test, y_test)
print(f"Loss: {loss}, Accuracy: {accuracy}")

# Prediksi dengan data baru
test_texts = ["I love this!", "This is awful.", "It's okay."]
test_texts_seq = tokenizer.texts_to_sequences(test_texts)
test_texts_padded = pad_sequences(test_texts_seq, maxlen=max_length, padding='post')

predictions = model.predict(test_texts_padded)
predicted_classes = tf.argmax(predictions, axis=1).numpy()

for text, sentiment in zip(test_texts, predicted_classes):
    print(f"Text: {text}")
    print(f"Predicted Sentiment: {['Negative', 'Neutral', 'Positive'][sentiment]}\n")

# Plot akurasi
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training and Validation Accuracy')
plt.show()

# Plot loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title('Training and Validation Loss')
plt.show()

converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
tflite_model = converter.convert()

# Simpan model TFLite
with open('sentimen.tflite', 'wb') as f:
    f.write(tflite_model)

# Build Pipeline
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# LogisticRegression Pipeline
pipe_lr = Pipeline(steps=[('tfidf', TfidfVectorizer()), ('lr', LogisticRegression(max_iter=200))])
# Train and Fit Data
pipe_lr.fit(x_train,y_train)

# Check Accuracy
pipe_lr.score(x_test,y_test)

# To Know the classes
pipe_lr.classes_

# Evaluasi model
from sklearn.metrics import classification_report, confusion_matrix

# Prediksi
y_pred = pipe_lr.predict(x_test)

# Classification Report
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=pipe_lr.classes_, yticklabels=pipe_lr.classes_)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# # Save Model & Pipeline
# import joblib
# pipeline_file = open("emotion_analysis.pkl","wb")
# joblib.dump(pipe_lr,pipeline_file)
# pipeline_file.close()